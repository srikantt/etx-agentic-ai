---
kind: ConfigMap
apiVersion: v1
metadata:
  name: llama-stack-config
data:
  run.yaml: |
    # Llama Stack configuration
    version: '2'
    image_name: vllm
    apis:
    - agents
    - inference
    - safety
    - tool_runtime
    - vector_io
    models:
      - metadata: {}
        model_id: granite-31-2b-instruct
        provider_id: vllm
        provider_model_id: granite-31-2b-instruct
        model_type: llm
    providers:
      agents:
      - provider_id: meta-reference
        provider_type: inline::meta-reference
        config:
          persistence_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/agents_store.db
          responses_store:
            type: sqlite
            db_path: ${env.SQLITE_STORE_DIR:=~/.llama/distributions/starter}/responses_store.db
      inference:
      - provider_id: vllm
        provider_type: "remote::vllm"
        config:
          url: "https://granite-31-2b-instruct.vllm-server.svc.cluster.local/v1"
          name: llama3.2:1b
          context_length: 4096
          tls_verify: false
      tool_runtime:
      - provider_id: tavily-search
        provider_type: remote::tavily-search
        config:
          api_key: ${env.TAVILY_API_KEY}
          max_results: 3
    tools:
      - name: builtin::websearch
        enabled: true
    tool_groups:
    - provider_id: tavily-search
      toolgroup_id: builtin::websearch
    server:
      port: 8321
